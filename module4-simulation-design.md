## 模块 4：基于 RL 的团伙 / 多账户协同攻防模拟设计

### 1. 模块总体目标与边界

- **模块定位**：
  - 构建一个**离线 / 准离线的攻防模拟与评估平台**，聚焦于**团伙 / 多账户协同欺诈场景**。
  - 通过 **强化学习（RL）+ 多 Agent 攻防环境**，其中防御侧复用现有 LLM+Agent 检测模块，攻击侧采用基于 **LLM+Agent 的策略模型（由 verl + rllm 等框架训练）**，主动生成高强度攻击样本，对现有防御体系进行压力测试和鲁棒性评估。

- **核心目标**：
  - **发现系统薄弱点**：找到快速模块 + LLM 模块 + 规则引擎在团伙欺诈场景下容易被打穿的模式。
  - **量化防御能力**：在不同防御策略 / 模型版本下，量化“在团伙攻击下的召回率、误报率与鲁棒性”。
  - **生成对抗样本库**：为后续模型对抗训练提供高价值的对抗样本（特别是多账户协同类型）。

- **非目标（Phase 1）**：
  - 不直接参与线上实时决策，不在交易流中做“阻断”。
  - 不承担生产监控指标展示（仅输出结构化结果，供数据分析 / 前端使用）。
  - 不在 Phase 1 实现完整前端页面（AttackSimulator / DefenseTest / BattleLog 可在后续阶段接入）。

---

### 2. 团伙 / 多账户协同攻击场景建模

#### 2.1 典型业务场景

- **团伙欺诈**：同一欺诈团伙操控多个账户，通过共享设备 / IP / 地址 / 银行卡等资源，并精心安排交易时序，以绕过单账户维度的检测。
- **多账户协同攻击模式示例**：
  - **资源共享型**：多个账户共享同一设备指纹、IP 地址、收货地址或银行卡，交替进行试探与大额交易。
  - **养号 + 爆雷型**：先用多个账户长期维持正常行为（小额、低风险交易），在某个时间窗口集中发起高风险交易。
  - **探路 + 复制型**：先用“探路账号”试探规则和模型边界，一旦发现漏洞，复制到大规模账户批量攻击。

#### 2.2 攻击者能力与约束假设（Phase 1）

- **攻击者能力**：
  - 能够批量注册多个账户（受限于某些约束，如设备/IP 上限）。
  - 能控制部分特征（controllable features），例如：交易金额、频率、时间分布、设备/IP 绑定关系、地区/商户选择等。
  - 能观察到防御系统的简化反馈：交易是否通过 / 被拒绝 / 进入审核，及部分可见信号（如被风控拦截）。

- **攻击者约束**：
  - 不能直接访问模型内部参数或完整特征；只能通过输入-输出行为“推断”决策边界。
  - 单个账户、单个设备/IP 的行为需要保持“看起来合理”，过于异常会导致快速被封禁（可以在奖励函数中对极端特征添加惩罚）。

#### 2.3 防御系统抽象

- 将现有防御能力抽象为统一的 **DefenseAdapter**：
  - 输入：一批交易请求（包含账户 ID、设备/IP、金额、时间等特征）。
  - 内部调用：
    - 快速模块（Fast 模块：45 维特征 + XGBoost + 规则前/后置）。
    - LLM+Agent 模块（仅针对灰区 / 指定样本触发）。
  - 输出：
    - `decision`：`accept / review / reject`。
    - `fraud_score`（0-1 或 0-1000 统一映射）。
    - `risk_level`：`low / medium / high`。
    - 其他可解释字段（规则命中、图特征异常、LLM 结论等）。

---

### 3. Phase 1：基于 RL 的团伙攻防环境设计

> 根据需求，Phase 1 直接引入 RL，不做“无 RL 预热阶段”。本节定义 Phase 1 就要落地的核心 RL 环境与训练框架。

#### 3.1 环境结构（FraudGroupAttackEnv）

- **环境角色**：
  - RL 智能体：`FraudsterGroupAgent`（基于 LLM+Agent 的团伙攻击者 RL 策略）。
  - 防御黑盒：`DefenseAdapter`（包装现有检测系统）。

- **环境交互流程（单 Episode 概要）**：
  1. 环境根据场景配置初始化一组“可用资源”：设备池、IP 池、初始账户数等。
  2. `FraudsterGroupAgent` 在每个时间步选择一个动作：
     - 例如“注册新账户并绑定某设备/IP”、“用某账户发起一笔交易（金额/时间/商户配置）”。
  3. 环境将本时间步产生的交易发送给 `DefenseAdapter`，获得防御系统输出。
  4. 根据攻击是否成功、防御代价、攻击代价等计算 Reward，并返回给 Agent。
  5. 满足终止条件（达到最大时长 / 攻击收益达到目标 / 资源用尽 / 被大规模封禁）时结束 Episode。

#### 3.2 状态空间（State）设计

State 需要兼顾：

- **团伙结构信息**：
  - 当前账户集合：每个账户的状态（是否活跃、历史交易次数、历史被拒绝次数、是否已“暴露”）。
  - 资源图结构：账户与设备 / IP / 地址等节点的连接关系（可以用简化的统计特征表示）。

- **行为摘要**：
  - 最近窗口内（例如最近 N 步）的交易行为统计：交易金额分布、通过 / 拒绝 / 审核比例。
  - 按设备/IP 聚合的行为强度：在某个设备/IP 上已有多少账户、交易次数等。

- **防御反馈摘要**：
  - Fast 模块输出的平均分数、风险等级分布。
  - LLM 模块触发比例、LLM 决策的通过/拒绝统计。

> 为了控制维度，Phase 1 可以先使用**聚合特征表示**（例如统计特征 + 简化图特征），后续再引入更细粒度的图结构输入。

#### 3.3 动作空间（Action）设计

Phase 1 动作空间采用 **离散高层动作 + 连续参数** 的混合设计（可先实现为“离散动作类型 + 参数向量”）：

- **动作类型（示例）**：
  - `REGISTER_ACCOUNT`：注册一个新账户，并选择设备/IP/地址绑定策略。
  - `BIND_RESOURCE`：调整某账户与设备/IP 的绑定（增加共享度）。
  - `NORMAL_TRANSACTION`：以“低风险模式”发起一笔小额、规律性交易（养号）。
  - `PROBING_TRANSACTION`：以“试探模式”轻微突破现有习惯特征，测试防御反应。
  - `ATTACK_TRANSACTION`：发起高风险交易，尝试大额套现或高欺诈收益。

- **动作参数（示例）**：
  - 选择哪一个账户 / 哪类账户（基于账户类别：老号/新号/高价值号等）。
  - 金额大小（连续）；时间间隔（连续）；商户类型（离散）。
  - 绑定的设备/IP（离散选择，受资源约束）。

在 RL 实现上，可以：

- 把动作拆分为：
  - 高层动作类型（离散），用策略网络输出 softmax。
  - 对应参数向量（连续），用策略网络输出均值+方差，按高斯分布采样。

#### 3.4 奖励函数（Reward）设计

Reward 需要平衡三个目标：**攻击成功率最大化、防御感知最小化、攻击成本最小化**。

- **正向奖励**：
  - 每次攻击交易被系统判定为 `accept` 即计为成功：
    - `R_success = +1 * attack_value_factor`（可按交易金额 / 预估获利来加权）。

- **负向奖励 / 惩罚**：
  - 攻击交易被 `reject` 或触发高风险规则：
    - `R_reject = -α`（α > 0，反映暴露风险）。
  - 账户被大量标记/封禁（可通过防御系统返回的某些标志模拟）：
    - 单次大惩罚 `R_ban = -β`（β >> α）。
  - 注册/维护大量账户和资源的成本：
    - 每注册一个账户、绑定一个新设备/IP，增加负向成本 `R_cost`。

- **团伙结构正则**：
  - 对过度集中的资源使用（例如某设备上挂载账户数过多）增加惩罚，鼓励 Agent 寻找更隐蔽的攻击拓扑。

- **总奖励示例**：

  \[
  R_t = w_1 \cdot R_{success} + w_2 \cdot R_{reject} + w_3 \cdot R_{ban} + w_4 \cdot R_{cost} + w_5 \cdot R_{structure}
  \]

  其中各个权重 \(w_i\) 可在实验配置中调整。

#### 3.5 Episode 设计与终止条件

- **Episode 时间尺度**：
  - 可模拟如“1 天 / 1 周内”的团伙行为，每个时间步对应 1~10 分钟或一笔交易事件。

- **终止条件**：
  - 达到最大时间步数 / 最大交易次数。
  - 团伙被防御系统“高度暴露”（例如大量账户被拒绝或封禁）。
  - 攻击累计收益达到设定阈值（模拟“攻击成功后收手”）。

- **Episode 输出**：
  - Episode 级别统计（攻击成功率、平均每账户收益、平均被拒绝率）。
  - 用于写入模拟结果表的聚合指标（见第 6 章）。

---

### 4. Agent 角色与实现规划

#### 4.1 FraudsterGroupAgent（团伙攻击者 LLM+RL Agent）

- **实现方式**：
  - 攻击侧采用 **LLM+Agent** 架构，由专门的 LLM 强化学习框架（例如 **verl + rllm**）驱动策略更新；
  - 环境仍然遵循 Gym 风格接口，与 `FraudGroupAttackEnv` 交互，底层可在 verl 中使用 PPO 等稳定的 policy gradient 算法。

- **输入**：
  - 环境 State（团伙结构 + 行为统计 + 防御反馈摘要）。

- **输出**：
  - 高层动作类型 + 对应参数向量。

- **训练产物**：
  - RL 策略模型（policy），标记版本号，用于后续模拟实验。

#### 4.2 DefenseAdapter（防御包装器）

- **职责**：将现有检测系统包装为统一接口，供 RL 环境调用。

- **内部调用**：
  - Fast 模块：基于 45 维特征 + XGBoost + 规则前置 & 后置。
  - LLM 模块：对于符合触发条件的样本，调用 LLM+Agent 决策，并返回 LLM 决策与解释。

- **接口设计**（示意）：

  - `def evaluate_transactions(transactions, config) -> List[DetectionResult]`
    - `transactions`：包含账户 ID、设备/IP、金额、时间、商户等信息的列表。
    - `config`：防御配置（模型版本、规则版本、LLM 策略等）。
    - `DetectionResult`：统一结构的检测结果，包含 `decision / fraud_score / risk_level / explanations`。

#### 4.3 攻防时序关系（Phase 1）

- **单步时序**：
  1. RL Agent 观察 state_t，输出 action_t。
  2. 环境将 action_t 转换为一系列具体操作（注册账户 / 发起交易）。
  3. 对于产生的交易，调用 DefenseAdapter，获得 detection_results_t。
  4. 环境据此计算 reward_t，并更新内部状态到 state_{t+1}。
  5. 如满足终止条件，则结束 Episode 并归档结果。

---

### 5. Phase 1 交付范围（重点：RL + 新增表）

根据约定，Phase 1 就要落地 RL 能力，重点包括：

- **必做项**：
  - 基于团伙 / 多账户协同场景的 `FraudGroupAttackEnv` 环境实现（简化版状态 / 动作）。
  - 基于 **verl + rllm** 驱动的 `FraudsterGroupAgent` LLM+RL 训练流程（放在 `scripts/` 下的训练脚本，可在内部使用 PPO 等算法）。
  - `DefenseAdapter` 封装，与现有 Fast / LLM 模块联通。
  - **新增一张模拟结果表**，用于记录每次模拟运行的聚合指标（详见第 6 章）。
  - 基础 CLI：
    - 训练 RL 策略：`python scripts/train_group_attack_rl.py ...`
    - 运行模拟实验：`python scripts/run_group_attack_simulation.py --policy-version=...`

- **可选项（若时间允许）**：
  - 简单 REST API，用于：
    - 查询已完成的模拟实验列表。
    - 按实验 ID 查询聚合指标与简要说明。

- **暂不纳入 Phase 1 的内容**：
  - 精细的 Battle Log 表结构（完整事件时间轴）——可先用 JSON 字段粗粒度记录。
  - 前端 AttackSimulator / DefenseTest 可视化页面。

---

### 6. Phase 1 新增数据库表设计（模拟结果表）

> 按需求，Phase 1 先**新增一张表**，用于记录每次团伙攻防模拟运行的结果汇总。后续若需要事件级别分析，再考虑拆分事件表。

#### 6.1 表名与用途

- 建议表名：`simulation_group_runs`
  - **用途**：
    - 记录每一次“团伙攻防模拟实验”的总体配置、RL 策略版本以及关键统计指标。
    - 支持按场景、防御配置、策略版本对比不同实验结果。

#### 6.2 字段设计（初稿）

> 下表为逻辑设计，可在真正建表时根据实际数据库（当前 SQLite / 未来 PostgreSQL）调整类型与约束。

- **主键与基础信息**：
  - `id`：主键，自增。
  - `run_id`：模拟运行 ID，字符串（可用于外部查询，例如 `SIM-2025-00123`）。
  - `scenario_name`：场景名称（例如 `"group_attack_basic"`）。
  - `description`：本次实验的简要说明（可选）。

- **防御配置相关**：
  - `defense_config`：JSON，记录防御策略配置（模型版本、规则版本、LLM 策略开关等）。

- **RL 策略与环境配置**：
  - `rl_algorithm`：字符串，例如 `"PPO"`、`"A2C"` 等，用于记录在 verl+rllm 训练过程中实际使用的底层 RL 算法。
  - `policy_version`：策略版本号（例如 `"ppo_group_v1"`）。
  - `env_config`：JSON，记录环境配置（最大账户数、设备/IP 池大小、episode 长度等）。

- **实验规模统计**：
  - `num_episodes`：本次模拟运行的 Episode 数量。
  - `total_steps`：总的时间步数 / 决策轮数。
  - `total_transactions`：总交易笔数（所有 Episode 累计）。

- **关键攻击/防御指标**：
  - `attack_success_rate`：攻击成功率（被系统判定为 `accept` 的攻击交易占所有攻击交易的比例）。
  - `avg_reward_per_episode`：每个 Episode 平均奖励。
  - `avg_steps_to_success`：达到某一攻击收益阈值所需的平均步数（如适用）。
  - `defense_recall_drop`：在模拟攻击下，防御召回率相对原始样本的下降幅度（如有基线可比较）。
  - `avg_accounts_used`：每个 Episode 平均涉及的账户数量（体现团伙规模）。

- **结构与行为摘要（JSON）**：
  - `structure_summary`：JSON，记录本次实验中团伙结构的统计（例如：平均每设备账户数分布、IP 共享度等）。
  - `behavior_summary`：JSON，本次实验中典型行为模式摘要（例如：常见时间分布、金额分布）。

- **审计与元数据**：
  - `started_at`：实验开始时间。
  - `finished_at`：实验结束时间。
  - `status`：`"pending" / "running" / "finished" / "failed"`。
  - `error_message`：若失败，记录失败原因。

> 说明：
> - Phase 1 不强制引入事件级别的明细表，可以在 `structure_summary` / `behavior_summary` 中以 JSON 方式记录部分代表性样本或统计结果。后续如要做 Battle Log 级别的可视化，再补充 `simulation_group_events` 等细粒度表。

---

### 7. 与现有模块的集成关系

#### 7.1 与模块 1（快速反欺诈）的关系

- 利用模块 1 提供的特征工程与快速打分能力：
  - RL 环境中生成的交易请求，经由 `DefenseAdapter` 调用快速模块完成特征提取 + 模型推理。
  - 模块 1 的决策输出（分数、规则命中、图特征异常）作为防御反馈的一部分进入 State 与 Reward 计算。

#### 7.2 与模块 2（LLM+Agent 智能反欺诈）的关系

- 对于进入灰区或指定策略要求的交易：
  - 防御适配器调用 LLM+Agent 工作流，得到更细致的风险评估与解释；
  - LLM 决策结果也会影响 Reward（例如被 LLM 拦截的攻击计入惩罚）。

#### 7.3 与模块 3（数据分析与监控）的关系

- Phase 1：
  - 通过新表 `simulation_group_runs` 直接对接模块 3，将模拟结果纳入分析视图（例如增加“在模拟团伙攻击下的鲁棒性曲线”）。
- 后续阶段：
  - 可在 analytics 模块中新增接口：
    - `GET /api/v1/analytics/simulation/group/runs`：查询模拟实验列表。
    - `GET /api/v1/analytics/simulation/group/runs/{run_id}`：查询某次模拟结果详情。

---

### 8. 后续 Phase 展望（简要）

> 虽然当前重点是 Phase 1，但为了避免设计走偏，这里简单预留后续演进方向。

- **Phase 2：事件级 Battle Log + 前端可视化**
  - 引入 `simulation_group_events` 等明细表，记录完整攻防时间轴。
  - 在前端实现 AttackSimulator / DefenseTest / BattleLog 页面。

- **Phase 3：多策略/多防御版本对比实验平台**
  - 支持同时对比多种防御策略（不同模型版本、规则集）。
  - 支持批量实验调度与自动报告生成。

- **Phase 4：闭环到模型与规则迭代**
  - 将高价值对抗样本自动加入训练数据集，支持对抗训练。
  - 对应规则的命中情况与攻击成功样本，输出给风控策略团队进行规则优化。

---

以上为模块 4（基于 RL 的团伙 / 多账户协同攻防模拟）的 Phase 1 详细设计初稿，已经对齐：
- 聚焦 **团伙 / 多账户协同** 场景；
- Phase 1 直接引入 **RL（PPO）环境与 Agent**；
- 新增一张模拟结果表 `simulation_group_runs` 用于落地实验级别指标。
