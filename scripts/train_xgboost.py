"""
XGBoost æ¨¡å‹è®­ç»ƒè„šæœ¬

ä½¿ç”¨ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹æ•°æ®é›†è®­ç»ƒ XGBoost åˆ†ç±»å™¨
"""
import sys
import os
from pathlib import Path
import time
import numpy as np
import polars as pl
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report, 
    confusion_matrix, 
    roc_auc_score, 
    precision_recall_curve,
    average_precision_score,
    roc_curve
)
from imblearn.over_sampling import SMOTE
import xgboost as xgb
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / 'backend'))
sys.path.insert(0, str(project_root))


class XGBoostTrainer:
    """XGBoost æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, data_dir: Path, model_dir: Path):
        self.data_dir = data_dir
        self.model_dir = model_dir
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        self.scaler = StandardScaler()
        self.model = None
        
    def load_data(self):
        """åŠ è½½é¢„å¤„ç†åçš„æ•°æ®"""
        print("ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®...")
        
        # åŠ è½½è®­ç»ƒé›†
        train_file = self.data_dir / "creditcard" / "train.npz"
        val_file = self.data_dir / "creditcard" / "val.npz"
        test_file = self.data_dir / "creditcard" / "test.npz"
        
        train_data = np.load(train_file)
        val_data = np.load(val_file)
        test_data = np.load(test_file)
        
        self.X_train = train_data['X']
        self.y_train = train_data['y']
        self.X_val = val_data['X']
        self.y_val = val_data['y']
        self.X_test = test_data['X']
        self.y_test = test_data['y']
        
        print(f"âœ… è®­ç»ƒé›†: {self.X_train.shape}, æ­£ä¾‹: {self.y_train.sum()}")
        print(f"âœ… éªŒè¯é›†: {self.X_val.shape}, æ­£ä¾‹: {self.y_val.sum()}")
        print(f"âœ… æµ‹è¯•é›†: {self.X_test.shape}, æ­£ä¾‹: {self.y_test.sum()}")
        
        # è®¡ç®—æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
        fraud_ratio = self.y_train.sum() / len(self.y_train)
        print(f"ğŸ“Š æ¬ºè¯ˆæ ·æœ¬æ¯”ä¾‹: {fraud_ratio*100:.3f}%")
        
        return fraud_ratio
    
    def load_synthetic_data(self, n_samples: int = 50000):
        """ä½¿ç”¨ç‰¹å¾å·¥ç¨‹æœåŠ¡ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ® (45ç»´ç‰¹å¾)"""
        print("ğŸ“‚ ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ® (45ç»´ç‰¹å¾)...")
        
        from app.db.database import SessionLocal, init_db
        from app.models.schemas import FraudDetectionRequest
        from app.services.feature_service import extract_features
        
        init_db()
        db = SessionLocal()
        
        X_list = []
        y_list = []
        rng = np.random.default_rng(42)
        
        for i in range(n_samples):
            # éšæœºç”Ÿæˆäº¤æ˜“åŸºæœ¬ä¿¡æ¯
            amount = float(np.clip(rng.lognormal(mean=8.0, sigma=1.0), 1.0, 200000.0))
            user_id = f"user_{rng.integers(0, 5000)}"
            device_id = f"device_{rng.integers(0, 2000)}"
            ip_address = f"10.{rng.integers(0, 255)}.{rng.integers(0, 255)}.{rng.integers(1, 254)}"
            merchant_id = f"merchant_{rng.integers(0, 500)}"
            merchant_category = f"MCC_{rng.integers(1000, 1100)}"
            transaction_type = "payment"
            location = "CN"
            
            request = FraudDetectionRequest(
                transaction_id=f"txn_{i}",
                user_id=user_id,
                amount=amount,
                merchant_id=merchant_id,
                merchant_category=merchant_category,
                device_id=device_id,
                ip_address=ip_address,
                location=location,
                transaction_type=transaction_type
            )
            
            # ä½¿ç”¨çº¿ä¸Šç‰¹å¾å·¥ç¨‹æå–45ç»´ç‰¹å¾
            features = extract_features(request, db, use_graph=True).astype(np.float32)
            
            if features.shape[0] != 45:
                raise ValueError(f"ç‰¹å¾ç»´åº¦é”™è¯¯, æœŸæœ›45, å®é™…{features.shape[0]}")
            
            # æ„é€ åˆæˆæ¬ºè¯ˆæ¦‚ç‡: é‡‘é¢ + å›¾ç‰¹å¾(é‚»å±…æ¬ºè¯ˆæ•°)
            fraud_neighbor_count = float(features[30])  # ç¬¬31ç»´: fraud_neighbor_count
            graph_risk = min(0.3, fraud_neighbor_count / 10.0)
            
            if amount < 2000:
                amount_risk = 0.02
            elif amount < 10000:
                amount_risk = 0.08
            elif amount < 50000:
                amount_risk = 0.18
            else:
                amount_risk = 0.35
            
            base_prob = 0.01
            fraud_prob = min(0.95, base_prob + amount_risk + graph_risk)
            
            label = 1 if rng.random() < fraud_prob else 0
            
            X_list.append(features)
            y_list.append(label)
        
        db.close()
        
        X = np.vstack(X_list)
        y = np.array(y_list, dtype=np.int32)
        
        print(f"âœ… åˆæˆæ•°æ®é›†å¤§å°: {X.shape}, æ¬ºè¯ˆæ ·æœ¬: {y.sum()}")
        
        # åˆ†å‰²ä¸ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
        X_train, X_temp, y_train, y_temp = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
        X_val, X_test, y_val, y_test = train_test_split(
            X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
        )
        
        self.X_train, self.y_train = X_train, y_train
        self.X_val, self.y_val = X_val, y_val
        self.X_test, self.y_test = X_test, y_test
        
        fraud_ratio = y_train.mean()
        print(f"ğŸ“Š è®­ç»ƒé›†æ¬ºè¯ˆæ ·æœ¬æ¯”ä¾‹: {fraud_ratio*100:.3f}%")
        
        return fraud_ratio
    
    def handle_imbalance(self, use_smote=True):
        """å¤„ç†æ ·æœ¬ä¸å¹³è¡¡é—®é¢˜"""
        if not use_smote:
            print("âš ï¸  ä¸ä½¿ç”¨ SMOTEï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®")
            return
        
        print("ğŸ”„ ä½¿ç”¨ SMOTE å¤„ç†æ ·æœ¬ä¸å¹³è¡¡...")
        start_time = time.time()
        
        # SMOTE è¿‡é‡‡æ ·
        smote = SMOTE(random_state=42)
        self.X_train, self.y_train = smote.fit_resample(self.X_train, self.y_train)
        
        elapsed = time.time() - start_time
        print(f"âœ… SMOTE å®Œæˆ - è€—æ—¶: {elapsed:.2f}ç§’")
        print(f"   æ–°è®­ç»ƒé›†å¤§å°: {self.X_train.shape}")
        print(f"   æ­£ä¾‹: {self.y_train.sum()}, è´Ÿä¾‹: {len(self.y_train) - self.y_train.sum()}")
    
    def train_model(self, use_smote=True, use_gpu=False):
        """è®­ç»ƒ XGBoost æ¨¡å‹"""
        print("\nğŸš€ å¼€å§‹è®­ç»ƒ XGBoost æ¨¡å‹...")
        
        # è®¡ç®—æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ï¼Œç”¨äº scale_pos_weight
        fraud_count = self.y_train.sum()
        normal_count = len(self.y_train) - fraud_count
        scale_pos_weight = normal_count / fraud_count if fraud_count > 0 else 1
        
        print(f"ğŸ“Š scale_pos_weight: {scale_pos_weight:.2f}")
        
        # XGBoost å‚æ•°
        params = {
            'objective': 'binary:logistic',
            'eval_metric': ['auc', 'logloss'],
            'max_depth': 6,
            'learning_rate': 0.1,
            'n_estimators': 200,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'min_child_weight': 1,
            'gamma': 0,
            'reg_alpha': 0.1,
            'reg_lambda': 1,
            'scale_pos_weight': scale_pos_weight if not use_smote else 1,
            'random_state': 42,
            'n_jobs': -1,
            'tree_method': 'gpu_hist' if use_gpu else 'hist',
        }
        
        # åˆ›å»ºæ¨¡å‹
        self.model = xgb.XGBClassifier(**params)
        
        # è®­ç»ƒ
        start_time = time.time()
        
        self.model.fit(
            self.X_train, 
            self.y_train,
            eval_set=[(self.X_val, self.y_val)],
            verbose=10
        )
        
        elapsed = time.time() - start_time
        print(f"âœ… è®­ç»ƒå®Œæˆ - è€—æ—¶: {elapsed/60:.2f}åˆ†é’Ÿ")
        
    def evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("\nğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        # åœ¨éªŒè¯é›†ä¸Šé¢„æµ‹
        y_pred_proba = self.model.predict_proba(self.X_val)[:, 1]
        
        # å°è¯•ä¸åŒçš„é˜ˆå€¼
        thresholds = [0.3, 0.5, 0.7]
        best_threshold = 0.5
        best_recall = 0
        
        print("\nğŸ” æµ‹è¯•ä¸åŒé˜ˆå€¼:")
        for threshold in thresholds:
            y_pred = (y_pred_proba >= threshold).astype(int)
            
            tn, fp, fn, tp = confusion_matrix(self.y_val, y_pred).ravel()
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
            
            print(f"\n  é˜ˆå€¼ = {threshold:.1f}")
            print(f"    å¬å›ç‡: {recall*100:.2f}%")
            print(f"    ç²¾ç¡®ç‡: {precision*100:.2f}%")
            print(f"    F1åˆ†æ•°: {f1:.4f}")
            print(f"    è¯¯æŠ¥ç‡: {fpr*100:.2f}%")
            
            # é€‰æ‹©å¬å›ç‡æœ€é«˜ä¸” >= 95% çš„é˜ˆå€¼
            if recall >= 0.95 and recall > best_recall:
                best_recall = recall
                best_threshold = threshold
        
        print(f"\nâœ… æœ€ä½³é˜ˆå€¼: {best_threshold} (å¬å›ç‡: {best_recall*100:.2f}%)")
        
        # ä½¿ç”¨æœ€ä½³é˜ˆå€¼åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
        y_test_proba = self.model.predict_proba(self.X_test)[:, 1]
        y_test_pred = (y_test_proba >= best_threshold).astype(int)
        
        print("\nğŸ“ˆ æµ‹è¯•é›†æœ€ç»ˆæ€§èƒ½:")
        print(classification_report(self.y_test, y_test_pred, 
                                   target_names=['æ­£å¸¸', 'æ¬ºè¯ˆ']))
        
        # è®¡ç®— AUC
        auc = roc_auc_score(self.y_test, y_test_proba)
        print(f"\nğŸ¯ AUC-ROC: {auc:.4f}")
        
        # è®¡ç®— Average Precision (PR-AUC)
        ap = average_precision_score(self.y_test, y_test_proba)
        print(f"ğŸ¯ Average Precision: {ap:.4f}")
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(self.y_test, y_test_pred)
        print(f"\næ··æ·†çŸ©é˜µ:")
        print(f"  TN: {cm[0,0]:6d}  |  FP: {cm[0,1]:6d}")
        print(f"  FN: {cm[1,0]:6d}  |  TP: {cm[1,1]:6d}")
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        metrics = {
            'best_threshold': float(best_threshold),
            'auc': float(auc),
            'average_precision': float(ap),
            'test_metrics': {
                'confusion_matrix': cm.tolist(),
                'classification_report': classification_report(
                    self.y_test, y_test_pred, 
                    target_names=['æ­£å¸¸', 'æ¬ºè¯ˆ'],
                    output_dict=True
                )
            }
        }
        
        return metrics, best_threshold
    
    def save_model(self, metrics: dict, threshold: float):
        """ä¿å­˜æ¨¡å‹å’Œç›¸å…³æ–‡ä»¶"""
        print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
        
        # ä¿å­˜ XGBoost æ¨¡å‹
        model_path = self.model_dir / "xgboost_model.json"
        self.model.save_model(model_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ä¿å­˜è¯„ä¼°æŒ‡æ ‡
        metrics_path = self.model_dir / "model_metrics.json"
        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)
        print(f"âœ… æŒ‡æ ‡å·²ä¿å­˜: {metrics_path}")
        
        # ä¿å­˜é˜ˆå€¼é…ç½®
        config = {
            'threshold': threshold,
            'model_type': 'xgboost',
            'features_count': self.X_train.shape[1],
            'trained_at': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        config_path = self.model_dir / "model_config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        print(f"âœ… é…ç½®å·²ä¿å­˜: {config_path}")
        
    def get_feature_importance(self, top_n=20):
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        print(f"\nğŸ“Š Top {top_n} é‡è¦ç‰¹å¾:")
        
        importance = self.model.feature_importances_
        indices = np.argsort(importance)[::-1][:top_n]
        
        for i, idx in enumerate(indices, 1):
            print(f"  {i:2d}. V{idx:2d}: {importance[idx]:.4f}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ¯ XGBoost åæ¬ºè¯ˆæ¨¡å‹è®­ç»ƒ")
    print("=" * 60)
    
    # è·¯å¾„é…ç½®
    data_dir = Path("data/processed")
    model_dir = Path("backend/models")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = XGBoostTrainer(data_dir, model_dir)
    
    # 1. ç”Ÿæˆ / åŠ è½½æ•°æ® (å½“å‰ä½¿ç”¨åˆæˆæ•°æ®, 45ç»´ç‰¹å¾)
    # fraud_ratio = trainer.load_data()
    fraud_ratio = trainer.load_synthetic_data(n_samples=50000)
    
    # 2. å¤„ç†ä¸å¹³è¡¡ (å¦‚æœæ¬ºè¯ˆæ¯”ä¾‹ < 1%ï¼Œä½¿ç”¨ SMOTE)
    use_smote = fraud_ratio < 0.01
    trainer.handle_imbalance(use_smote=use_smote)
    
    # 3. è®­ç»ƒæ¨¡å‹
    trainer.train_model(use_smote=use_smote, use_gpu=False)
    
    # 4. è¯„ä¼°æ¨¡å‹
    metrics, threshold = trainer.evaluate_model()
    
    # 5. ä¿å­˜æ¨¡å‹
    trainer.save_model(metrics, threshold)
    
    # 6. ç‰¹å¾é‡è¦æ€§
    trainer.get_feature_importance()
    
    print("\n" + "=" * 60)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
