"""
æ•°æ®é¢„å¤„ç†è„šæœ¬
å¤„ç† Credit Cardã€YelpChi å’Œ Amazon æ•°æ®é›†
"""

import polars as pl
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import scipy.io as sio
import pickle
import json

# è®¾ç½®è·¯å¾„
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
RAW_DIR = DATA_DIR / "raw"
GRAPH_DIR = DATA_DIR / "graph"
PROCESSED_DIR = DATA_DIR / "processed"

# åˆ›å»ºè¾“å‡ºç›®å½•
PROCESSED_DIR.mkdir(parents=True, exist_ok=True)
(PROCESSED_DIR / "creditcard").mkdir(exist_ok=True)
(PROCESSED_DIR / "graph").mkdir(exist_ok=True)


def process_creditcard_data():
    """
    å¤„ç† Kaggle Credit Card Fraud Detection æ•°æ®é›†
    """
    print("\n" + "="*60)
    print("ğŸ“Š å¤„ç† Credit Card æ•°æ®é›†")
    print("="*60)
    
    # 1. è¯»å–æ•°æ®
    print("\n1ï¸âƒ£ è¯»å–æ•°æ®...")
    csv_path = RAW_DIR / "creditcard" / "creditcard.csv"
    
    # ä½¿ç”¨ Polars è¯»å–ï¼Œæ˜ç¡®æŒ‡å®š Time å’Œ Amount ä¸ºæµ®ç‚¹æ•°
    df = pl.read_csv(
        str(csv_path),
        schema_overrides={
            "Time": pl.Float64,
            "Amount": pl.Float64
        }
    )
    
    print(f"   âœ… æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"   ğŸ“Š æ€»äº¤æ˜“æ•°: {len(df):,}")
    print(f"   âš ï¸  æ¬ºè¯ˆäº¤æ˜“: {df.filter(pl.col('Class') == 1).shape[0]:,}")
    print(f"   âœ… æ­£å¸¸äº¤æ˜“: {df.filter(pl.col('Class') == 0).shape[0]:,}")
    
    fraud_rate = df.filter(pl.col('Class') == 1).shape[0] / len(df) * 100
    print(f"   ğŸ“ˆ æ¬ºè¯ˆç‡: {fraud_rate:.4f}%")
    
    # 2. åŸºç¡€ç»Ÿè®¡
    print("\n2ï¸âƒ£ æ•°æ®ç»Ÿè®¡...")
    print(df.describe())
    
    # 3. ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆAmount éœ€è¦æ ‡å‡†åŒ–ï¼ŒV1-V28 å·²ç» PCA å¤„ç†è¿‡ï¼‰
    print("\n3ï¸âƒ£ ç‰¹å¾æ ‡å‡†åŒ–...")
    
    # è½¬æ¢ä¸º NumPy è¿›è¡Œæ ‡å‡†åŒ–
    df_np = df.to_numpy()
    
    # Amount åœ¨å€’æ•°ç¬¬äºŒåˆ—
    scaler = StandardScaler()
    df_np[:, -2] = scaler.fit_transform(df_np[:, -2].reshape(-1, 1)).flatten()
    
    # Time ç‰¹å¾è½¬æ¢ï¼ˆè½¬ä¸ºå°æ—¶ï¼‰
    df_np[:, 0] = df_np[:, 0] / 3600  # ç§’è½¬å°æ—¶
    
    # ä¿å­˜ scaler
    scaler_path = PROCESSED_DIR / "creditcard" / "amount_scaler.pkl"
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    print(f"   âœ… Scaler å·²ä¿å­˜: {scaler_path.name}")
    
    # 4. åˆ†å‰²æ•°æ®é›†
    print("\n4ï¸âƒ£ åˆ†å‰²æ•°æ®é›†...")
    
    X = df_np[:, :-1]  # æ‰€æœ‰ç‰¹å¾
    y = df_np[:, -1]   # æ ‡ç­¾
    
    # åˆ†å±‚æŠ½æ ·ï¼ˆä¿æŒæ¬ºè¯ˆç‡ä¸€è‡´ï¼‰
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
    )
    
    print(f"   ğŸ“Š è®­ç»ƒé›†: {len(X_train):,} (æ¬ºè¯ˆ: {int(y_train.sum()):,})")
    print(f"   ğŸ“Š éªŒè¯é›†: {len(X_val):,} (æ¬ºè¯ˆ: {int(y_val.sum()):,})")
    print(f"   ğŸ“Š æµ‹è¯•é›†: {len(X_test):,} (æ¬ºè¯ˆ: {int(y_test.sum()):,})")
    
    # 5. ä¿å­˜å¤„ç†åçš„æ•°æ®
    print("\n5ï¸âƒ£ ä¿å­˜æ•°æ®...")
    
    # ä¿å­˜ä¸º NumPy æ ¼å¼ï¼ˆè®­ç»ƒç”¨ï¼‰
    np.savez_compressed(
        PROCESSED_DIR / "creditcard" / "train.npz",
        X=X_train, y=y_train
    )
    np.savez_compressed(
        PROCESSED_DIR / "creditcard" / "val.npz",
        X=X_val, y=y_val
    )
    np.savez_compressed(
        PROCESSED_DIR / "creditcard" / "test.npz",
        X=X_test, y=y_test
    )
    
    # ä¿å­˜ç‰¹å¾åç§°
    feature_names = df.columns[:-1]  # é™¤äº† Class
    with open(PROCESSED_DIR / "creditcard" / "feature_names.json", 'w') as f:
        json.dump(list(feature_names), f, indent=2)
    
    print(f"   âœ… è®­ç»ƒé›†å·²ä¿å­˜: train.npz")
    print(f"   âœ… éªŒè¯é›†å·²ä¿å­˜: val.npz")
    print(f"   âœ… æµ‹è¯•é›†å·²ä¿å­˜: test.npz")
    
    # 6. ç”Ÿæˆæ•°æ®æ‘˜è¦
    summary = {
        "dataset": "Credit Card Fraud Detection",
        "total_samples": int(len(df)),
        "fraud_samples": int(df.filter(pl.col('Class') == 1).shape[0]),
        "normal_samples": int(df.filter(pl.col('Class') == 0).shape[0]),
        "fraud_rate": float(fraud_rate),
        "features": len(feature_names),
        "train_size": int(len(X_train)),
        "val_size": int(len(X_val)),
        "test_size": int(len(X_test)),
        "feature_names": list(feature_names)
    }
    
    with open(PROCESSED_DIR / "creditcard" / "summary.json", 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f"   âœ… æ•°æ®æ‘˜è¦å·²ä¿å­˜: summary.json")
    
    print("\nâœ… Credit Card æ•°æ®å¤„ç†å®Œæˆ!")
    return summary


def process_graph_data():
    """
    å¤„ç†å›¾æ•°æ®é›† (YelpChi å’Œ Amazon)
    """
    print("\n" + "="*60)
    print("ğŸ“Š å¤„ç†å›¾æ•°æ®é›†")
    print("="*60)
    
    datasets = {
        "yelp": GRAPH_DIR / "CARE-GNN" / "data" / "yelp" / "YelpChi.mat",
        "amazon": GRAPH_DIR / "CARE-GNN" / "data" / "amazon" / "Amazon.mat"
    }
    
    summaries = {}
    
    for name, mat_path in datasets.items():
        print(f"\n{'='*60}")
        print(f"ğŸ“Š å¤„ç† {name.upper()} æ•°æ®é›†")
        print(f"{'='*60}")
        
        # 1. åŠ è½½ .mat æ–‡ä»¶
        print(f"\n1ï¸âƒ£ åŠ è½½æ•°æ®: {mat_path.name}")
        mat_data = sio.loadmat(str(mat_path))
        
        # 2. æå–æ•°æ®
        print("\n2ï¸âƒ£ æå–å›¾ç»“æ„...")
        
        # ä¸åŒæ•°æ®é›†çš„é”®åå¯èƒ½ä¸åŒ
        if name == "yelp":
            features = mat_data.get('features', None)
            labels = mat_data.get('label', None)
            homo_adj = mat_data.get('homo', None)
            
        elif name == "amazon":
            features = mat_data.get('features', None)
            labels = mat_data.get('label', None)
            homo_adj = mat_data.get('homo', None)
        
        # 3. æ•°æ®ç»Ÿè®¡
        print("\n3ï¸âƒ£ æ•°æ®ç»Ÿè®¡...")
        if features is not None:
            print(f"   ğŸ“Š èŠ‚ç‚¹æ•°é‡: {features.shape[0]:,}")
            print(f"   ğŸ“Š ç‰¹å¾ç»´åº¦: {features.shape[1]:,}")
        
        if labels is not None:
            labels_flat = labels.flatten()
            fraud_count = int((labels_flat == 1).sum())
            normal_count = int((labels_flat == 0).sum())
            fraud_rate = fraud_count / len(labels_flat) * 100
            
            print(f"   âš ï¸  æ¬ºè¯ˆèŠ‚ç‚¹: {fraud_count:,}")
            print(f"   âœ… æ­£å¸¸èŠ‚ç‚¹: {normal_count:,}")
            print(f"   ğŸ“ˆ æ¬ºè¯ˆç‡: {fraud_rate:.2f}%")
        
        if homo_adj is not None:
            # è®¡ç®—è¾¹æ•°ï¼ˆç¨€ç–çŸ©é˜µï¼‰
            if hasattr(homo_adj, 'nnz'):
                edge_count = homo_adj.nnz
            else:
                edge_count = np.count_nonzero(homo_adj)
            print(f"   ğŸ”— è¾¹æ•°é‡: {edge_count:,}")
        
        # 4. ä¿å­˜å¤„ç†åçš„æ•°æ®
        print("\n4ï¸âƒ£ ä¿å­˜æ•°æ®...")
        
        output_dir = PROCESSED_DIR / "graph" / name
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ä¸º NumPy æ ¼å¼
        if features is not None:
            np.save(output_dir / "features.npy", features)
            print(f"   âœ… ç‰¹å¾å·²ä¿å­˜: features.npy")
        
        if labels is not None:
            np.save(output_dir / "labels.npy", labels)
            print(f"   âœ… æ ‡ç­¾å·²ä¿å­˜: labels.npy")
        
        if homo_adj is not None:
            # ä¿å­˜ç¨€ç–çŸ©é˜µ
            from scipy.sparse import save_npz
            if hasattr(homo_adj, 'tocsr'):
                save_npz(output_dir / "adj_matrix.npz", homo_adj.tocsr())
            else:
                save_npz(output_dir / "adj_matrix.npz", homo_adj)
            print(f"   âœ… é‚»æ¥çŸ©é˜µå·²ä¿å­˜: adj_matrix.npz")
        
        # 5. ç”Ÿæˆæ‘˜è¦
        summary = {
            "dataset": name,
            "num_nodes": int(features.shape[0]) if features is not None else 0,
            "num_features": int(features.shape[1]) if features is not None else 0,
            "num_edges": int(edge_count) if homo_adj is not None else 0,
            "fraud_count": int(fraud_count) if labels is not None else 0,
            "normal_count": int(normal_count) if labels is not None else 0,
            "fraud_rate": float(fraud_rate) if labels is not None else 0.0
        }
        
        with open(output_dir / "summary.json", 'w') as f:
            json.dump(summary, f, indent=2)
        
        print(f"   âœ… æ‘˜è¦å·²ä¿å­˜: summary.json")
        
        summaries[name] = summary
        
        print(f"\nâœ… {name.upper()} æ•°æ®å¤„ç†å®Œæˆ!")
    
    return summaries


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*60)
    print("ğŸš€ åæ¬ºè¯ˆç³»ç»Ÿ - æ•°æ®é¢„å¤„ç†")
    print("="*60)
    
    # å¤„ç† Credit Card æ•°æ®
    cc_summary = process_creditcard_data()
    
    # å¤„ç†å›¾æ•°æ®
    graph_summaries = process_graph_data()
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ“Š é¢„å¤„ç†æ€»ç»“")
    print("="*60)
    
    print("\nâœ… Credit Card æ•°æ®é›†:")
    print(f"   - æ€»æ ·æœ¬: {cc_summary['total_samples']:,}")
    print(f"   - è®­ç»ƒé›†: {cc_summary['train_size']:,}")
    print(f"   - éªŒè¯é›†: {cc_summary['val_size']:,}")
    print(f"   - æµ‹è¯•é›†: {cc_summary['test_size']:,}")
    print(f"   - æ¬ºè¯ˆç‡: {cc_summary['fraud_rate']:.4f}%")
    
    print("\nâœ… å›¾æ•°æ®é›†:")
    for name, summary in graph_summaries.items():
        print(f"\n   {name.upper()}:")
        print(f"   - èŠ‚ç‚¹æ•°: {summary['num_nodes']:,}")
        print(f"   - è¾¹æ•°: {summary['num_edges']:,}")
        print(f"   - æ¬ºè¯ˆç‡: {summary['fraud_rate']:.2f}%")
    
    # ä¿å­˜æ€»ä½“æ‘˜è¦
    all_summary = {
        "creditcard": cc_summary,
        "graph": graph_summaries
    }
    
    with open(PROCESSED_DIR / "processing_summary.json", 'w') as f:
        json.dump(all_summary, f, indent=2)
    
    print("\n" + "="*60)
    print("ğŸ‰ æ‰€æœ‰æ•°æ®é¢„å¤„ç†å®Œæˆ!")
    print("="*60)
    print(f"\nğŸ“ å¤„ç†åçš„æ•°æ®ä½ç½®: {PROCESSED_DIR}")
    print("\nä¸‹ä¸€æ­¥:")
    print("  1. æ•°æ®æ¢ç´¢åˆ†æ: python scripts/analyze_data.py")
    print("  2. è®­ç»ƒåŸºç¡€æ¨¡å‹: python scripts/train_model.py")
    print("  3. æ„å»ºå›¾æ•°æ®åº“: python scripts/build_graph_db.py")


if __name__ == "__main__":
    main()
